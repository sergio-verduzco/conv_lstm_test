{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv_lstm.ipynb\n",
    "\n",
    "A convolutional LSTM that predicts the next image for the block manipulation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from jax import random\n",
    "from jax.tree_util import tree_map\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import math\n",
    "from functools import partial\n",
    "from jax.tree_util import Partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# 64x64\n",
    "#path = \"/mnt/bucket/TaniU/Members/prasanna/dataset_5x8/5x6comp1train_dataset0901.h5\"\n",
    "# 256x256\n",
    "path = \"/mnt/bucket/TaniU/Members/prasanna/prasanna_data/datasets/groupA1_traindataset_256x256.h5\"\n",
    "#path = \"/mnt/bucket/TaniU/Members/prasanna/prasanna_data/datasets/groupA1_testdataset_256x256.h5\"\n",
    "\n",
    "# quick information\n",
    "f = h5py.File(path, 'r')\n",
    "keys = list(f.keys())\n",
    "for key in keys:\n",
    "    print(f\"'{key}' \\t dataset shape: {f[key].shape}\")\n",
    "\n",
    "# dataset class\n",
    "class JaxDataset(data.Dataset):\n",
    "    \"\"\" A Torch Dataset class with the HDF5 datasets.\n",
    "\n",
    "    :param data_path: path of the HDF5 dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        f = h5py.File(data_path, 'r')\n",
    "        self.lang_mask = f['lang_mask']\n",
    "        self.language = f['language']\n",
    "        self.mask = f['mask']\n",
    "        self.motor = f['motor']\n",
    "        self.vision = f['vision']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vision.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns the training data corresponding to 'index'.\n",
    "\n",
    "        :param index: index of the data to return\n",
    "        :type index: int\n",
    "        :return: vision, motor, language, mask, lang_mask\n",
    "        :rtype: tuple(Numpy array)\n",
    "        \"\"\"\n",
    "        return (self.vision[index], self.motor[index], self.language[index],\n",
    "                self.mask[index], self.lang_mask[index])\n",
    "\n",
    "dataset = JaxDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "  return tree_map(np.asarray, data.default_collate(batch))\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "  def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "    super(self.__class__, self).__init__(dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        sampler=sampler,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=numpy_collate,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        timeout=timeout,\n",
    "        worker_init_fn=worker_init_fn)\n",
    "\n",
    "data_loader = NumpyLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data_loader object\n",
    "for i, datum in enumerate(data_loader):\n",
    "    vision, motor, language, mask, lang_mask = datum\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"Vision shape: {vision.shape}. \\t Vision type: {type(vision)}\")\n",
    "    print(f\"Motor shape: {motor.shape}. \\t Motor type: {type(motor)}\")\n",
    "    print(f\"Language shape: {language.shape}. \\t Language type: {type(language)}.\")\n",
    "    print(f\"Mask shape: {mask.shape}. \\t Mask type: {type(mask)}.\")\n",
    "    print(f\"Lang_mask shape: {lang_mask.shape}. \\t Lang_mask type: {type(lang_mask)}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_lstm_forward(params, x, h, c, conv_params):\n",
    "    \"\"\" A functional implementation of a convolutional LSTM.\n",
    "    \n",
    "        :param params: parameters of the model. A dictionary with the following keys:\n",
    "                       'wxi': kernel for input gate input convolution,\n",
    "                       'whi': kernel for input gate hidden state convolution,\n",
    "                       'bi': bias for input gate,\n",
    "                       'wxf': kernel for forget gate input convolution,\n",
    "                       'whf': kernel for forget gate hidden state convolution,\n",
    "                       'bf': bias for forget gate,\n",
    "                       'wxo': kernel for output gate input convolution,\n",
    "                       'who': kernel for output gate hidden state convolution,\n",
    "                       'bo': bias for output gate\n",
    "                       'wxg': kernel for cell state input convolution,\n",
    "                       'whg': kernel for cell state hidden state convolution,\n",
    "                       'bg': bias for cell state,\n",
    "        :type params: dict(str, jnp.array)\n",
    "        :param x: input to the LSTM cell\n",
    "        :type x: jnp.array\n",
    "        :param h: hidden state of the LSTM cell\n",
    "        :type h: jnp.array\n",
    "        :param c: cell state of the LSTM cell\n",
    "        :type c: jnp.array\n",
    "        :param conv_params: convolution hyperparameters dictionary with these entries:\n",
    "                        'strides': tuple(int) with the strides for the input convolutions,\n",
    "                        'padding': (str | Sequence[tuple(int, int)]) with padding\n",
    "                        'lhs_dilation': tuple(int) with the input dilation\n",
    "                        'rhs_dilation': tuple(int) with the kernel dilation\n",
    "                        'dnx': dimension numbers for input convolutions,\n",
    "                        'dnh': dimension numbers for hidden state convolutions,\n",
    "        :returns: new hidden state and cell state of the LSTM cell\n",
    "        :rtype: tuple(jnp.array)\n",
    "    \"\"\"\n",
    "    s = conv_params['strides']\n",
    "    p = conv_params['padding']\n",
    "    ld = conv_params['lhs_dilation']\n",
    "    rd = conv_params['rhs_dilation']\n",
    "    dnx = conv_params['dnx']\n",
    "    dnh = conv_params['dnh']\n",
    "    # i = sigmoid(Conv(x, wxi) + Conv(h, whi) + bi)\n",
    "    i = jax.nn.sigmoid(lax.conv_general_dilated(x, params['wxi'], s, p, ld, rd, dnx) +\n",
    "                       lax.conv_general_dilated(h,\n",
    "                                                params['whi'],\n",
    "                                                (1, 1),\n",
    "                                                'SAME',\n",
    "                                                (1, 1),\n",
    "                                                (1, 1), \n",
    "                                                dnh) + params['bi'][:, None, None])\n",
    "    # f = sigmoid(Conv(x, wxf) + Conv(h, whf) + bf)\n",
    "    f = jax.nn.sigmoid(lax.conv_general_dilated(x, params['wxf'], s, p, ld, rd, dnx) +\n",
    "                       lax.conv_general_dilated(h,\n",
    "                                                params['whf'],\n",
    "                                                (1, 1),\n",
    "                                                'SAME',\n",
    "                                                (1, 1),\n",
    "                                                (1, 1),\n",
    "                                                dnh) + params['bf'][:, None, None])\n",
    "    # o = sigmoid(Conv(x, wxo) + Conv(h, who) + bo)\n",
    "    o = jax.nn.sigmoid(lax.conv_general_dilated(x, params['wxo'], s, p, ld, rd, dnx) +\n",
    "                       lax.conv_general_dilated(h,\n",
    "                                                params['who'],\n",
    "                                                (1, 1),\n",
    "                                                'SAME',\n",
    "                                                (1, 1),\n",
    "                                                (1, 1),\n",
    "                                                dnh) + params['bo'][:, None, None])\n",
    "    # g = tanh(Conv(x, wxg) + Conv(h, whg) + bg)\n",
    "    g = jax.lax.tanh(lax.conv_general_dilated(x, params['wxg'], s, p, ld, rd, dnx) +\n",
    "                     lax.conv_general_dilated(h,\n",
    "                                              params['whg'],\n",
    "                                              (1, 1),\n",
    "                                              'SAME',\n",
    "                                              (1, 1),\n",
    "                                              (1, 1),\n",
    "                                              dnh) + params['bg'][:, None, None])\n",
    "    c = f * c + i * g\n",
    "    h = o * jax.lax.tanh(c)\n",
    "\n",
    "    return h, c\n",
    "\n",
    "\n",
    "def padding2(input_dim, output_dim, stride, kernel_size, dilation, inp_dilation):\n",
    "    \"\"\"Calculate the padding for a convolution.\n",
    "\n",
    "    The padding is calculated to attain the desired input and output\n",
    "    dimensions. If an asymmetric padding is needed, the function will\n",
    "    return a non-zero residual value r.\n",
    "\n",
    "    You can specify the asymmetric padding in JAX's conv_general_dilated by using\n",
    "    a padding argument like: ((pad + r, pad), (pad + r, pad))\n",
    "\n",
    "    :param input_dim: size of the input along the relevant dimension\n",
    "    :type input_dim: int\n",
    "    :param output_dim: height or width of the convolution output\n",
    "    :type output_dim: int\n",
    "    :param stride: stride\n",
    "    :type stride: int\n",
    "    :param kernel_size: kernel size\n",
    "    :type kernel_size: int\n",
    "    :param dilation: dilation\n",
    "    :type dilation: int\n",
    "    :param inp_dilation: dilation of the input\n",
    "    :type inp_dilation: int\n",
    "    :returns: padding for the convolution, residual value\n",
    "    :rtype: int, int\n",
    "    \"\"\"\n",
    "    pad = 0.5 * (stride * (output_dim - 1) - input_dim - (input_dim - 1) * (inp_dilation - 1)\n",
    "             + dilation * (kernel_size - 1) + 1)\n",
    "    r = math.ceil(pad % 1)\n",
    "    return math.floor(pad), r\n",
    "\n",
    "\n",
    "def prediction_step(params, x, h, c, conv_params, conv_params_t):\n",
    "    \"\"\" A step of the convLSTM including a predicted next input.\n",
    "    \n",
    "        :param params: as in conv_lstm_forward, with the following extra entries:\n",
    "                       'whx': kernel for hidden state to input convolution,\n",
    "        :param x: input to the LSTM cell\n",
    "        :type x: jnp.array\n",
    "        :param h: hidden state of the LSTM cell\n",
    "        :type h: jnp.array\n",
    "        :param c: cell state of the LSTM cell\n",
    "        :type c: jnp.array\n",
    "        :param conv_params: as in conv_lstm_forward\n",
    "        :type conv_params: dict(str, tuple) | dict(str, ConvDimensionNumbers)\n",
    "        :param conv_params_t: hyperparameters dictionary for the transposed convolution:\n",
    "                    'strides': tuple(int) with the strides for the input convolutions,\n",
    "                    'padding': (str | Sequence[tuple(int, int)]) with padding\n",
    "                    'lhs_dilation': tuple(int) with the input dilation\n",
    "                    'rhs_dilation': tuple(int) with the kernel dilation\n",
    "                    'dnt': dimension numbers for transpose convolution\n",
    "        :type conv_params_t: dict(str, tuple) | dict(str, ConvDimensionNumbers)\n",
    "        :returns: new hidden state, cell state of the LSTM cell, and predicted x\n",
    "        :rtype: tuple(jnp.array)\n",
    "    \"\"\"\n",
    "    h, c = conv_lstm_forward(params, x, h, c, conv_params)\n",
    "    next_x = lax.conv_general_dilated(h,\n",
    "                                      params['whx'],\n",
    "                                      conv_params_t['strides'],\n",
    "                                      conv_params_t['padding'],\n",
    "                                      conv_params_t['lhs_dilation'],\n",
    "                                      conv_params_t['rhs_dilation'],\n",
    "                                      conv_params_t['dnt'],\n",
    "                                     )\n",
    "    return h, c, next_x\n",
    "\n",
    "\n",
    "def prediction_n_steps(params, vision, h, c, conv_params, conv_params_t):\n",
    "    \"\"\" All temporal steps of autoregressive prediction with a convolutional LSTM.\n",
    "\n",
    "        :param params: parameters of the model, as in `prediction_step`\n",
    "        :type params: dict(str, jnp.array) | dict(str, ConvDimensionNumbers)\n",
    "        :param vision: first input to the LSTM cell\n",
    "        :type vision: jnp.array with shape (batches, T, channels, height, width)\n",
    "        :param h: initial hidden state of the LSTM cell\n",
    "        :type h: jnp.array\n",
    "        :param c: initial cell state of the LSTM cell\n",
    "        :type c: jnp.array\n",
    "        :returns: array with the predicted vision data\n",
    "        :rtype: jnp.array with shape (batches, T, channels, height, width)\n",
    "    \"\"\"\n",
    "    n = vision.shape[1]  # This will prevent jitting\n",
    "    x_pred = jnp.zeros_like(vision)\n",
    "    x = vision[:, 0, :, :, :]\n",
    "    x_pred = x_pred.at[:, 0, :, :, :].set(x)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        h, c, x = prediction_step(params, x, h, c, conv_params, conv_params_t)\n",
    "        x_pred = x_pred.at[:, i, :, :, :].set(x)\n",
    "\n",
    "    return x_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the filters and biases\n",
    "h_channels = 12  # number of channels in the hidden state\n",
    "kernel_shape_x = (3, h_channels, 3, 3)  # kernel for input convolutions\n",
    "kernel_shape_h = (h_channels, h_channels, 3, 3)  # for hidden state convolutions\n",
    "kernel_shape_t = (h_channels, 3, 5, 5)  # kernel for the transpose convolution\n",
    "# kernel_shape_x = (n_input_channels, n_output_channels, height, width)\n",
    "bias_shape = (h_channels,)\n",
    "strides = (2, 2)  # stride for direct convolutions\n",
    "pad = ((0, 0), (0, 0))  # padding for direct convolutions\n",
    "inp_dilation = (1, 1)  # dilation for the input in the direct convolutions\n",
    "ker_dilation = (1, 1)  # kernel dilation in the direct convolutions\n",
    "\n",
    "# initialize the parameters\n",
    "def kernel_initializer(key, shape):\n",
    "    \"\"\" Randomly initialize the kernel for the convolutions.\n",
    "\n",
    "        :param key: random number generator\n",
    "        :type key: jax.random key\n",
    "        :param shape: shape of the kernel as (IOHW)\n",
    "        :type shape: tuple(int)\n",
    "        :returns: initialized kernel\n",
    "        :rtype: jnp.array\n",
    "    \"\"\"\n",
    "    return jax.random.normal(key, shape) / np.sqrt(np.prod(shape[2:]))\n",
    "\n",
    "def bias_initializer(key, shape):\n",
    "    \"\"\"\" Randomly initialize the bias for the convolutions.\n",
    "\n",
    "         :param key: random number generator\n",
    "         :type key : jax.random key\n",
    "         :param shape: shape of the bias array\n",
    "         :type length: int\n",
    "         :returns: bias array\n",
    "         :rtype: jnp.array\n",
    "    \"\"\"\n",
    "    return jax.random.normal(key, shape) / np.sqrt(np.prod(shape))\n",
    "\n",
    "gate_names = ['i', 'f', 'o', 'g']\n",
    "key = random.key(3)\n",
    "key, *all_keys = random.split(key, 2 * len(gate_names) + 1)\n",
    "params = {}\n",
    "\n",
    "for idx, name in enumerate(gate_names):\n",
    "    key_x = all_keys[2 * idx]\n",
    "    key_h = all_keys[2 * idx + 1]\n",
    "    params['wx' + name] = kernel_initializer(key_x, kernel_shape_x)\n",
    "    params['wh' + name] = kernel_initializer(key_h, kernel_shape_h)\n",
    "\n",
    "key = random.key(34)\n",
    "key, *all_keys = random.split(key, len(gate_names) + 1)\n",
    "\n",
    "for name, key in zip(gate_names, all_keys):\n",
    "    params['b' + name] = bias_initializer(key, bias_shape)\n",
    "\n",
    "# vision has shape (N, T, C, H, W). We will remove the temporal dimension T.\n",
    "dnx = lax.conv_dimension_numbers((vision.shape[0],) + vision.shape[2:],\n",
    "                                 kernel_shape_x,     # only ndim matters, not shape \n",
    "                                ('NCHW', 'IOHW', 'NCHW'))  # dimension meanings\n",
    "# calculate the shape of the hidden state\n",
    "h_shape = jax.eval_shape(partial(lax.conv_general_dilated, window_strides=strides,\n",
    "                    padding=pad, lhs_dilation=inp_dilation, rhs_dilation=ker_dilation,\n",
    "                    dimension_numbers=dnx), vision[:, 0, :, :, :], params['wxi']).shape\n",
    "\n",
    "print(f\"hidden state shape: {h_shape}\")\n",
    "print(\"(n_batch, n_channels, height, width)\")\n",
    "\n",
    "# dimension numbers for the hidden state recursive convolutions\n",
    "dnh = lax.conv_dimension_numbers(h_shape, kernel_shape_h, ('NCHW', 'IOHW', 'NCHW')) \n",
    "\n",
    "# kernel for the transposed convolution\n",
    "key, key_t = random.split(key)\n",
    "params['whx'] = kernel_initializer(key_t, kernel_shape_t)\n",
    "\n",
    "# Create parameters for the transpose convolution (assume square input and output)\n",
    "strides_t = (1, 1)  # stride for the transpose convolution\n",
    "inp_dilation_t = strides  # dilation for the input in the transpose convolution\n",
    "ker_dilation_t = (1, 1)  # kernel dilation in the transpose convolution\n",
    "p_t, r_t = padding2(h_shape[2],\n",
    "                    vision.shape[3],\n",
    "                    strides_t[0],\n",
    "                    kernel_shape_t[2],\n",
    "                    ker_dilation[0],\n",
    "                    inp_dilation_t[0])\n",
    "pad_t = ((p_t + r_t, p_t), (p_t + r_t, p_t))  # padding for the transpose convolution\n",
    "\n",
    "dnt = lax.conv_dimension_numbers(h_shape, kernel_shape_t, ('NCHW', 'IOHW', 'NCHW'))\n",
    "\n",
    "conv_params = {'strides': strides,\n",
    "               'padding': pad,\n",
    "               'lhs_dilation': inp_dilation,\n",
    "               'rhs_dilation': ker_dilation,\n",
    "               'dnx': dnx,\n",
    "               'dnh': dnh,\n",
    "              }\n",
    "conv_params_t = {'strides': strides_t,\n",
    "                 'padding': pad_t,\n",
    "                 'lhs_dilation': inp_dilation_t,\n",
    "                 'rhs_dilation': ker_dilation_t,\n",
    "                 'dnt': dnt,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of conv_lstm_forward\n",
    "\n",
    "# initialize the hidden state and cell state\n",
    "key = random.key(345)\n",
    "key_h, key_c = random.split(key)\n",
    "h = 0.1 * jax.random.normal(key_h, h_shape)\n",
    "c = 0.1 * jax.random.normal(key_c, h_shape)\n",
    "\n",
    "# apply conv_lstm_forward\n",
    "h, c = conv_lstm_forward(params, vision[:,0, :, :, :], h, c, conv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of prediction_step\n",
    "\n",
    "# initialize the hidden state and cell state\n",
    "key = random.key(345)\n",
    "key_h, key_c = random.split(key)\n",
    "h = 0.1 * jax.random.normal(key_h, h_shape)\n",
    "c = 0.1 * jax.random.normal(key_c, h_shape)\n",
    "\n",
    "# apply prediction_step\n",
    "h, c, next_x = prediction_step(params, vision[:,0, :, :, :], h, c, conv_params, conv_params_t)\n",
    "\n",
    "h, c, next_x = prediction_step(params, next_x, h, c, conv_params, conv_params_t)\n",
    "\n",
    "print(f\"Hidden state shape: {h.shape}\")\n",
    "print(f\"Cell state shape: {c.shape}\")\n",
    "print(f\"Predicted x shape: {next_x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of prediction_n_steps\n",
    "\n",
    "# initialize the hidden state and cell state\n",
    "key = random.key(345)\n",
    "key_h, key_c = random.split(key)\n",
    "h = 0.1 * jax.random.normal(key_h, h_shape)\n",
    "c = 0.1 * jax.random.normal(key_c, h_shape)\n",
    "\n",
    "# apply prediction_n_steps\n",
    "x_pred = prediction_n_steps(params, vision, h, c, conv_params, conv_params_t)\n",
    "\n",
    "print(f\"x_pred shape: {x_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_lstm_forward(params, x, h, c, conv_params):\n",
    "    \"\"\" A functional implementation of a convolutional LSTM.\n",
    "    \n",
    "        :param params: parameters of the model. A named tuple with following fields:\n",
    "                       'wxi': kernel for input gate input convolution,\n",
    "                       'whi': kernel for input gate hidden state convolution,\n",
    "                       'bi': bias for input gate,\n",
    "                       'wxf': kernel for forget gate input convolution,\n",
    "                       'whf': kernel for forget gate hidden state convolution,\n",
    "                       'bf': bias for forget gate,\n",
    "                       'wxo': kernel for output gate input convolution,\n",
    "                       'who': kernel for output gate hidden state convolution,\n",
    "                       'bo': bias for output gate\n",
    "                       'wxg': kernel for cell state input convolution,\n",
    "                       'whg': kernel for cell state hidden state convolution,\n",
    "                       'bg': bias for cell state,\n",
    "        :type params: NamedTuple\n",
    "        :param x: input to the LSTM cell\n",
    "        :type x: jnp.array\n",
    "        :param h: hidden state of the LSTM cell\n",
    "        :type h: jnp.array\n",
    "        :param c: cell state of the LSTM cell\n",
    "        :type c: jnp.array\n",
    "        :param conv_params: convolution hyperparameters NamedTuple with these entries:\n",
    "                        'strides': tuple(int) with the strides for the input convolutions,\n",
    "                        'padding': (str | Sequence[tuple(int, int)]) with padding\n",
    "                        'lhs_dilation': tuple(int) with the input dilation\n",
    "                        'rhs_dilation': tuple(int) with the kernel dilation\n",
    "                        'dnx': dimension numbers for input convolutions,\n",
    "                        'dnh': dimension numbers for hidden state convolutions,\n",
    "        :returns: new hidden state and cell state of the LSTM cell\n",
    "        :rtype: tuple(jnp.array)\n",
    "    \"\"\"\n",
    "    s = conv_params.strides\n",
    "    p = conv_params.padding\n",
    "    ld = conv_params.lhs_dilation\n",
    "    rd = conv_params.rhs_dilation\n",
    "    dnx = conv_params.dnx\n",
    "    dnh = conv_params.dnh\n",
    "    # i = sigmoid(Conv(x, wxi) + Conv(h, whi) + bi)\n",
    "    i = jax.nn.sigmoid(lax.conv_general_dilated(x, params.wxi, s, p, ld, rd, dnx) +\n",
    "                       lax.conv_general_dilated(h,\n",
    "                                                params.whi,\n",
    "                                                (1, 1),\n",
    "                                                'SAME',\n",
    "                                                (1, 1),\n",
    "                                                (1, 1), \n",
    "                                                dnh) + params.bi[:, None, None])\n",
    "    # f = sigmoid(Conv(x, wxf) + Conv(h, whf) + bf)\n",
    "    f = jax.nn.sigmoid(lax.conv_general_dilated(x, params.wxf, s, p, ld, rd, dnx) +\n",
    "                       lax.conv_general_dilated(h,\n",
    "                                                params.whf,\n",
    "                                                (1, 1),\n",
    "                                                'SAME',\n",
    "                                                (1, 1),\n",
    "                                                (1, 1),\n",
    "                                                dnh) + params.bf[:, None, None])\n",
    "    # o = sigmoid(Conv(x, wxo) + Conv(h, who) + bo)\n",
    "    o = jax.nn.sigmoid(lax.conv_general_dilated(x, params.wxo, s, p, ld, rd, dnx) +\n",
    "                       lax.conv_general_dilated(h,\n",
    "                                                params.who,\n",
    "                                                (1, 1),\n",
    "                                                'SAME',\n",
    "                                                (1, 1),\n",
    "                                                (1, 1),\n",
    "                                                dnh) + params.bo[:, None, None])\n",
    "    # g = tanh(Conv(x, wxg) + Conv(h, whg) + bg)\n",
    "    g = jax.lax.tanh(lax.conv_general_dilated(x, params.wxg, s, p, ld, rd, dnx) +\n",
    "                     lax.conv_general_dilated(h,\n",
    "                                              params.whg,\n",
    "                                              (1, 1),\n",
    "                                              'SAME',\n",
    "                                              (1, 1),\n",
    "                                              (1, 1),\n",
    "                                              dnh) + params.bg[:, None, None])\n",
    "    c = f * c + i * g\n",
    "    h = o * jax.lax.tanh(c)\n",
    "\n",
    "    return h, c\n",
    "\n",
    "\n",
    "def padding2(input_dim, output_dim, stride, kernel_size, dilation, inp_dilation):\n",
    "    \"\"\"Calculate the padding for a convolution.\n",
    "\n",
    "    The padding is calculated to attain the desired input and output\n",
    "    dimensions. If an asymmetric padding is needed, the function will\n",
    "    return a non-zero residual value r.\n",
    "\n",
    "    You can specify the asymmetric padding in JAX's conv_general_dilated by using\n",
    "    a padding argument like: ((pad + r, pad), (pad + r, pad))\n",
    "\n",
    "    :param input_dim: size of the input along the relevant dimension\n",
    "    :type input_dim: int\n",
    "    :param output_dim: height or width of the convolution output\n",
    "    :type output_dim: int\n",
    "    :param stride: stride\n",
    "    :type stride: int\n",
    "    :param kernel_size: kernel size\n",
    "    :type kernel_size: int\n",
    "    :param dilation: dilation\n",
    "    :type dilation: int\n",
    "    :param inp_dilation: dilation of the input\n",
    "    :type inp_dilation: int\n",
    "    :returns: padding for the convolution, residual value\n",
    "    :rtype: int, int\n",
    "    \"\"\"\n",
    "    pad = 0.5 * (stride * (output_dim - 1) - input_dim - (input_dim - 1) * (inp_dilation - 1)\n",
    "             + dilation * (kernel_size - 1) + 1)\n",
    "    r = math.ceil(pad % 1)\n",
    "    return math.floor(pad), r\n",
    "\n",
    "\n",
    "def prediction_step(params, x, h, c, conv_params, conv_params_t):\n",
    "    \"\"\" A step of the convLSTM including a predicted next input.\n",
    "    \n",
    "        :param params: as in conv_lstm_forward, with the following extra entries:\n",
    "                       'whx': kernel for hidden state to input convolution,\n",
    "        :param x: input to the LSTM cell\n",
    "        :type x: jnp.array\n",
    "        :param h: hidden state of the LSTM cell\n",
    "        :type h: jnp.array\n",
    "        :param c: cell state of the LSTM cell\n",
    "        :type c: jnp.array\n",
    "        :param conv_params: as in conv_lstm_forward\n",
    "        :type conv_params: NamedTuple\n",
    "        :param conv_params_t: hyperparameters named tuple for the transposed convolution:\n",
    "                    'strides': tuple(int) with the strides for the input convolutions,\n",
    "                    'padding': (str | Sequence[tuple(int, int)]) with padding\n",
    "                    'lhs_dilation': tuple(int) with the input dilation\n",
    "                    'rhs_dilation': tuple(int) with the kernel dilation\n",
    "                    'dnt': dimension numbers for transpose convolution\n",
    "        :type conv_params_t: NamedTuple\n",
    "        :returns: new hidden state, cell state of the LSTM cell, and predicted x\n",
    "        :rtype: tuple(jnp.array)\n",
    "    \"\"\"\n",
    "    h, c = conv_lstm_forward(params, x, h, c, conv_params)\n",
    "    next_x = lax.conv_general_dilated(h,\n",
    "                                      params.whx,\n",
    "                                      conv_params_t.strides,\n",
    "                                      conv_params_t.padding,\n",
    "                                      conv_params_t.lhs_dilation,\n",
    "                                      conv_params_t.rhs_dilation,\n",
    "                                      conv_params_t.dnt,\n",
    "                                     )\n",
    "    return h, c, next_x\n",
    "\n",
    "\n",
    "def prediction_n_steps(params, vision, h, c, conv_params, conv_params_t):\n",
    "    \"\"\" All temporal steps of autoregressive prediction with a convolutional LSTM.\n",
    "\n",
    "        All batches are done in parallel.\n",
    "\n",
    "        :param params: parameters of the model, as in `prediction_step`\n",
    "        :type params: NamedTuple\n",
    "        :param vision: All inputs to the LSTM cell\n",
    "        :type vision: jnp.array with shape (batches, T, channels, height, width)\n",
    "        :param h: initial hidden state of the LSTM cell\n",
    "        :type h: jnp.array\n",
    "        :param c: initial cell state of the LSTM cell\n",
    "        :type c: jnp.array\n",
    "        :param conv_params: as in conv_lstm_forward\n",
    "        :type conv_params: NamedTuple\n",
    "        :param conv_params_t: in prediction_step\n",
    "        :type conv_params_t: NamedTuple\n",
    "        :returns: array with the predicted vision data\n",
    "        :rtype: jnp.array with shape (batches, T, channels, height, width)\n",
    "    \"\"\"\n",
    "    n = vision.shape[1]  # This will prevent jitting\n",
    "    x_pred = jnp.zeros_like(vision)\n",
    "    x = vision[:, 0, :, :, :]\n",
    "    x_pred = x_pred.at[:, 0, :, :, :].set(x)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        h, c, x = prediction_step(params, x, h, c, conv_params, conv_params_t)\n",
    "        x_pred = x_pred.at[:, i, :, :, :].set(x)\n",
    "\n",
    "    return x_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters in named tuples\n",
    "from typing import NamedTuple\n",
    "\n",
    "class ConvLSTMParams(NamedTuple):\n",
    "    wxi: jnp.array\n",
    "    whi: jnp.array\n",
    "    bi: jnp.array\n",
    "    wxf: jnp.array\n",
    "    whf: jnp.array\n",
    "    bf: jnp.array\n",
    "    wxo: jnp.array\n",
    "    who: jnp.array\n",
    "    bo: jnp.array\n",
    "    wxg: jnp.array\n",
    "    whg: jnp.array\n",
    "    bg: jnp.array\n",
    "    whx: jnp.array\n",
    "\n",
    "class ConvParams(NamedTuple):\n",
    "    strides: tuple\n",
    "    padding: tuple\n",
    "    lhs_dilation: tuple\n",
    "    rhs_dilation: tuple\n",
    "    dnx: tuple\n",
    "    dnh: tuple\n",
    "\n",
    "class ConvParamsT(NamedTuple):\n",
    "    strides: tuple\n",
    "    padding: tuple\n",
    "    lhs_dilation: tuple\n",
    "    rhs_dilation: tuple\n",
    "    dnt: tuple\n",
    "\n",
    "def params_to_nt(params, conv_params, conv_params_t):\n",
    "    \"\"\" Convert the parameters to named tuples.\n",
    "\n",
    "        :param params: parameters of the model\n",
    "        :type params: dict(str, jnp.array)\n",
    "        :param conv_params: convolution hyperparameters dictionary\n",
    "        :type conv_params: dict(str, tuple) | dict(str, ConvDimensionNumbers)\n",
    "        :param conv_params_t: hyperparameters\n",
    "        :type conv_params_t: dict(str, tuple) | dict(str, ConvDimensionNumbers)\n",
    "        :returns: named tuples with the parameters\n",
    "        :rtype: tuple(NamedTuple)\n",
    "    \"\"\"\n",
    "    return (ConvLSTMParams(**{k: v for k, v in params.items()}),\n",
    "            ConvParams(**{k: v for k, v in conv_params.items()}),\n",
    "            ConvParamsT(**{k: v for k, v in conv_params_t.items()}))\n",
    "\n",
    "params_nt, conv_params_nt, conv_params_t_nt = params_to_nt(params, conv_params, conv_params_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, vision, h, c, conv_params, conv_params_t):\n",
    "    \"\"\" Calculate the loss of the model.\n",
    "\n",
    "        :param params: parameters of the model\n",
    "        :type params: dict(str, jnp.array)\n",
    "        :param vision: input data\n",
    "        :type vision: jnp.array\n",
    "        :param h: initial hidden state\n",
    "        :type h: jnp.array\n",
    "        :param c: initial cell state\n",
    "        :type c: jnp.array\n",
    "        :param conv_params: convolution hyperparameters dictionary\n",
    "        :type conv_params: dict(str, tuple) | dict(str, ConvDimensionNumbers)\n",
    "        :param conv_params_t: hyperparameters dictionary for the transposed convolution\n",
    "        :type conv_params_t: dict(str, tuple) | dict(str, ConvDimensionNumbers)\n",
    "        :returns: loss value\n",
    "        :rtype: jnp.array\n",
    "    \"\"\"\n",
    "    x_pred = prediction_n_steps(params, vision, h, c, conv_params, conv_params_t)\n",
    "    return jnp.mean((x_pred - vision) ** 2)\n",
    "\n",
    "@Partial(jax.jit, static_argnums=(4, 5))\n",
    "def sgd_update(params, vision, h, c, conv_params, conv_params_t, lr=1e-3):\n",
    "    \"\"\" Update the parameters of the model using SGD.\n",
    "\n",
    "        :param params: parameters of the model\n",
    "        :type params: dict(str, jnp.array)\n",
    "        :param vision: input data\n",
    "        :type vision: jnp.array\n",
    "        :param h: initial hidden state\n",
    "        :type h: jnp.array\n",
    "        :param c: initial cell state\n",
    "        :type c: jnp.array\n",
    "        :param conv_params: convolution hyperparameters dictionary\n",
    "        :type conv_params: dict(str, tuple) | dict(str, ConvDimensionNumbers)\n",
    "        :param conv_params_t: hyperparameters dictionary for the transposed convolution\n",
    "        :type conv_params_t: dict(str, tuple) | dict(str, ConvDimensionNumbers)\n",
    "        :param lr: learning rate\n",
    "        :type lr: float\n",
    "        :returns: loss value, new optimizer state and new parameters\n",
    "        :rtype: tuple(jnp.array, dict, dict)\n",
    "    \"\"\"\n",
    "    loss_val, grads = jax.value_and_grad(loss)(params, vision, h, c, conv_params, conv_params_t)\n",
    "    new_params = jax.tree.map(\n",
    "        lambda p, g: p - lr * g, params, grads\n",
    "    )\n",
    "    return loss_val, new_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "\n",
    "# initialize the hidden state and cell state\n",
    "key = random.key(23456)\n",
    "key_h, key_c = random.split(key)\n",
    "h = 0.1 * jax.random.normal(key_h, h_shape)\n",
    "c = 0.1 * jax.random.normal(key_c, h_shape)\n",
    "lr = 5e-4  # learning rate\n",
    "n_steps = 2000  # number of optimization steps\n",
    "# losses = []\n",
    "\n",
    "# Run training epochs\n",
    "for i in range(n_steps):\n",
    "    loss_val, params_nt = sgd_update(params_nt, vision, h, c, conv_params_nt, conv_params_t_nt, lr)\n",
    "    losses.append(loss_val)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i}, loss: {loss_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5m 14s for 100 epochs without jit\n",
    "\n",
    "2m 52s for 100 epochs with jit (no warmup)  \n",
    "102m for 4000 epochs with jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some predictions\n",
    "\n",
    "prediction = prediction_n_steps(params_nt, vision, h, c, conv_params_nt, conv_params_t_nt)\n",
    "pt_prediction = torch.from_numpy(np.asarray(prediction)) # don't use in real code\n",
    "example_index = 3 \n",
    "vision_ex = pt_prediction[example_index, :, :, :, :]\n",
    "print(f\"For index {example_index}, the vision data has shape {vision_ex.shape}\")\n",
    "\n",
    "imgs = vision_ex / 2. + 0.5\n",
    "grid = make_grid(torch.tensor(imgs))\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(10,10))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
