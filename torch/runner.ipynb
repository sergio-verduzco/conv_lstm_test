{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858e3823-c2a9-4ce4-8617-a218466d98c3",
   "metadata": {},
   "source": [
    "# runner.ipynb\n",
    "\n",
    "A notebook to use the methods in `torch_conv_lstm.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5511417-6a3f-4f2a-b156-ede7d1d093fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch_conv_lstm import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83266a-561d-4de0-ba5a-17d5052c0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/z/Data/datasets/language_network/groupA1_traindataset_256x256.h5\"\n",
    "config = read_config('rnn_config.yaml')\n",
    "\n",
    "data_loader = TorchDataLoader(path, config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eaa700-5b65-475a-9e96-1fbd853f1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = next(data_loader._get_iterator())\n",
    "vision, motor, language, mask, lang_mask = datum\n",
    "print(f\"Vision shape: {vision.shape}. \\t Vision type: {type(vision)}\")\n",
    "print(f\"Motor shape: {motor.shape}. \\t Motor type: {type(motor)}\")\n",
    "print(f\"Language shape: {language.shape}. \\t Language type: {type(language)}.\")\n",
    "print(f\"Mask shape: {mask.shape}. \\t Mask type: {type(mask)}.\")\n",
    "print(f\"Lang_mask shape: {lang_mask.shape}. \\t Lang_mask type: {type(lang_mask)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfda1a4-ff84-40b4-b409-95b52f3f5577",
   "metadata": {},
   "source": [
    "---\n",
    "Test the `ConvLSTMCell` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bb02d-e7fa-4573-8e50-b9a6cf66c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test conv_lstm cell\n",
    "inp_channels = 3\n",
    "hid_size = 128  # desired hidden state size\n",
    "pad, r  = padding_fun(vision.shape[3],\n",
    "                      hid_size,\n",
    "                      config['s'],\n",
    "                      config['inp_kernel_size'],\n",
    "                      config['kd'])\n",
    "\n",
    "conv_lstm = ConvLSTMCell(inp_channels,\n",
    "                         config['h_channels'],\n",
    "                         config['inp_kernel_size'],\n",
    "                         config['hid_kernel_size'],\n",
    "                         inp_stride=config['s'],\n",
    "                         inp_padding=pad,\n",
    "                         ik_dilation=config['kd'])\n",
    "\n",
    "image_batch = vision[:, 0, :, :, :].to(torch.float32)\n",
    "\n",
    "img_height = image_batch.shape[2]\n",
    "img_width = image_batch.shape[3]\n",
    "h, c = conv_lstm.init_hidden_from_normal(vision.shape[0], (img_height, img_width))\n",
    "\n",
    "print(f\"h shape: {h.shape}\")\n",
    "print(f\"c shape: {c.shape}\")\n",
    "\n",
    "h, c = conv_lstm(image_batch, (h, c))\n",
    "\n",
    "print(f\"h shape: {h.shape}\")\n",
    "print(f\"c shape: {c.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63e397-0460-4f35-8381-9562e855a1a8",
   "metadata": {},
   "source": [
    "---\n",
    "Test the `PredictorCell` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b93c3-1096-48c3-8ecc-9c0fc658e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 3  # the RGB channels\n",
    "hid_size = 128  # desired hidden state size\n",
    "pad, r  = padding_fun(vision.shape[3],\n",
    "                      hid_size,\n",
    "                      config['s'],\n",
    "                      config['inp_kernel_size'],\n",
    "                      config['kd'])\n",
    "conv_params = {'input_channels': input_channels,\n",
    "               'hidden_channels': config['h_channels'],\n",
    "               'inp_kernel_size': config['inp_kernel_size'],\n",
    "               'hid_kernel_size': config['hid_kernel_size'],\n",
    "               'inp_stride': config['s'],\n",
    "               'inp_padding': pad,\n",
    "               'ik_dilation': config['kd'],\n",
    "               'bias': True,\n",
    "              }\n",
    "\n",
    "pad_t, r_t = padding_fun(vision.shape[3],\n",
    "                         hid_size,\n",
    "                         config['s'],\n",
    "                         config['trans_kernel_size'],\n",
    "                         1)\n",
    "conv_params_t = {'kernel_size': config['trans_kernel_size'],\n",
    "                 'ik_dilation': 1,\n",
    "                 'inp_padding': pad_t,\n",
    "                 'output_padding': r_t,\n",
    "                 'bias': True,\n",
    "                }\n",
    "\n",
    "pred_cell = PredictorCell(conv_params, conv_params_t)\n",
    "\n",
    "image_batch = vision[:, 0, :, :, :].to(torch.float32)\n",
    "\n",
    "img_height = image_batch.shape[2]\n",
    "img_width = image_batch.shape[3]\n",
    "h, c = pred_cell.init_hidden(vision.shape[0], (img_height, img_width))\n",
    "\n",
    "x_next, h, c = pred_cell(image_batch, (h, c))\n",
    "\n",
    "print(f\"h shape: {h.shape}\")\n",
    "print(f\"c shape: {c.shape}\")\n",
    "print(f\"x_next shape: {x_next.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d380886-4300-4443-965e-c234870f5406",
   "metadata": {},
   "source": [
    "---\n",
    "Test the `Predictor` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9504f-3d97-43e9-bf30-6b7161cbae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 3  # the RGB channels\n",
    "hid_size = 128  # desired hidden state size\n",
    "T = vision.shape[1]  # number of images to predict\n",
    "pad, r  = padding_fun(vision.shape[3],\n",
    "                      hid_size,\n",
    "                      config['s'],\n",
    "                      config['inp_kernel_size'],\n",
    "                      config['kd'])\n",
    "conv_params = {'input_channels': input_channels,\n",
    "               'hidden_channels': config['h_channels'],\n",
    "               'inp_kernel_size': config['inp_kernel_size'],\n",
    "               'hid_kernel_size': config['hid_kernel_size'],\n",
    "               'inp_stride': config['s'],\n",
    "               'inp_padding': pad,\n",
    "               'ik_dilation': config['kd'],\n",
    "               'bias': True,\n",
    "              }\n",
    "\n",
    "pad_t, r_t = padding_fun(vision.shape[3],\n",
    "                         hid_size,\n",
    "                         config['s'],\n",
    "                         config['trans_kernel_size'],\n",
    "                         1)\n",
    "conv_params_t = {'kernel_size': config['trans_kernel_size'],\n",
    "                 'ik_dilation': 1,\n",
    "                 'inp_padding': pad_t,\n",
    "                 'output_padding': r_t,\n",
    "                 'bias': True,\n",
    "                }\n",
    "image_batch = vision[:, 0, :, :, :].to(torch.float32)\n",
    "\n",
    "predictor = Predictor(conv_params, conv_params_t)\n",
    "\n",
    "pred_sequence = predictor(image_batch, T)\n",
    "\n",
    "print(f\"image_batch shape: {image_batch.shape}\")\n",
    "print(f\"pred_sequence shape: {pred_sequence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb8fcc-79cb-4819-8610-ac39dc1057d6",
   "metadata": {},
   "source": [
    "---\n",
    "### Perform backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e959588-38d6-4b45-aa32-623cd5a2911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network\n",
    "\n",
    "n_epochs = config['n_epochs']\n",
    "input_channels = 3  # the RGB channels. This is not configurable.\n",
    "hid_size = 128  # desired hidden state size\n",
    "T = vision.shape[1]  # number of images to predict\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "pad, r  = padding_fun(vision.shape[3],\n",
    "                      hid_size,\n",
    "                      config['s'],\n",
    "                      config['inp_kernel_size'],\n",
    "                      config['kd'])\n",
    "conv_params = {'input_channels': input_channels,\n",
    "               'hidden_channels': config['h_channels'],\n",
    "               'inp_kernel_size': config['inp_kernel_size'],\n",
    "               'hid_kernel_size': config['hid_kernel_size'],\n",
    "               'inp_stride': config['s'],\n",
    "               'inp_padding': pad,\n",
    "               'ik_dilation': config['kd'],\n",
    "               'bias': True,\n",
    "              }\n",
    "\n",
    "pad_t, r_t = padding_fun(vision.shape[3],\n",
    "                         hid_size,\n",
    "                         config['s'],\n",
    "                         config['trans_kernel_size'],\n",
    "                         1)\n",
    "conv_params_t = {'kernel_size': config['trans_kernel_size'],\n",
    "                 'ik_dilation': 1,\n",
    "                 'inp_padding': pad_t,\n",
    "                 'output_padding': r_t,\n",
    "                 'bias': True,\n",
    "                }\n",
    "\n",
    "predictor = Predictor(conv_params, conv_params_t).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(predictor.parameters(), lr=config['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b00755-4fa6-4f4e-90fe-22e742e468ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the epochs\n",
    "\n",
    "if 'losses' not in locals():\n",
    "    losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "warmup_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    accum_loss = 0.0\n",
    "    if epoch == warmup_epochs:\n",
    "        start_time = time.time()\n",
    "    for datum in data_loader:\n",
    "        vision, motor, language, mask, lang_mask = datum\n",
    "        vision = vision.to(device, dtype=torch.float32)\n",
    "        first_images_batch = vision[:, 0, :, :, :].detach().clone().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = predictor(first_images_batch, vision.shape[1])\n",
    "        loss = loss_fn(vision, predictions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accum_loss += loss.item()\n",
    "    losses.append(accum_loss / len(data_loader))\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={losses[-1]}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Completed {n_epochs - warmup_epochs} epochs in {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d47ca-1cd5-4b98-9ea0-6476020d6942",
   "metadata": {},
   "source": [
    "2 epochs in 78 seconds with no compilation.  \n",
    "2 epochs in 132 seconds with \"inductor\" compilation, no warmup, batch_size=32.  \n",
    "2 epochs in 133 seconds with \"inductor\" compilation, 1 epoch warmup, batch_size=32.  \n",
    "2 epochs in 53 seconds with \"cudagraphs\" compilation, 1 epoch warmup, batch_size=16.\n",
    "3 epochs in 79 seconds with \"cudagraphs\" compilation, 1 epoch warmup, batch_size=16.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362978a-909d-41d3-8f2f-aa35277bd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some predictions\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "\n",
    "example_index = 1\n",
    "datum = next(data_loader._get_iterator())\n",
    "vision, motor, language, mask, lang_mask = datum\n",
    "vision = vision.to(device, dtype=torch.float32)\n",
    "first_images_batch = vision[:, 0, :, :, :].detach().clone().to(device)\n",
    "predictions = predictor(first_images_batch, vision.shape[1])\n",
    "vision_ex = predictions[example_index, :, :, :, :]\n",
    "\n",
    "\n",
    "print(f\"For index {example_index}, the vision data has shape {vision_ex.shape}\")\n",
    "\n",
    "imgs = vision_ex / 2. + 0.5\n",
    "grid = make_grid(torch.tensor(imgs))\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(10,10))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba873e-da9f-47b6-80e2-72c0f0ab3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0ce81-33de-4d56-9305-f619eb88aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d47927-812f-496a-8cb0-d84cce45108c",
   "metadata": {},
   "source": [
    "---\n",
    "Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1ee32-ee5f-4673-9107-d5dcc8aed3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = config['model_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d36876-1e31-4d85-98f5-7cd6238b94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and optimizer state dictionaries along with other info\n",
    "torch.save({\n",
    "    'model_state_dict': predictor.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'losses': losses,\n",
    "}, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7613b89-f592-416a-bb3d-64294109ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint (run after first cell of backpropagation)\n",
    "checkpoint = torch.load(save_name)\n",
    "predictor.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "losses = checkpoint['losses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ed7c1-4b6a-4873-905b-5b989bcf9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "26 / 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab70d4-7891-41aa-aad2-ab6501a011d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
