{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858e3823-c2a9-4ce4-8617-a218466d98c3",
   "metadata": {},
   "source": [
    "# runner.ipynb\n",
    "\n",
    "A notebook to use the methods in `jax_conv_lstm.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5511417-6a3f-4f2a-b156-ede7d1d093fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch_conv_lstm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e83266a-561d-4de0-ba5a-17d5052c0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/z/Data/datasets/language_network/groupA1_traindataset_256x256.h5\"\n",
    "config = read_config('rnn_config.yaml')\n",
    "\n",
    "data_loader = TorchDataLoader(path, config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05eaa700-5b65-475a-9e96-1fbd853f1d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision shape: torch.Size([35, 50, 3, 256, 256]). \t Vision type: <class 'torch.Tensor'>\n",
      "Motor shape: torch.Size([35, 50, 60]). \t Motor type: <class 'torch.Tensor'>\n",
      "Language shape: torch.Size([35, 5, 20]). \t Language type: <class 'torch.Tensor'>.\n",
      "Mask shape: torch.Size([35, 50]). \t Mask type: <class 'torch.Tensor'>.\n",
      "Lang_mask shape: torch.Size([35, 5]). \t Lang_mask type: <class 'torch.Tensor'>.\n"
     ]
    }
   ],
   "source": [
    "datum = next(data_loader._get_iterator())\n",
    "vision, motor, language, mask, lang_mask = datum\n",
    "print(f\"Vision shape: {vision.shape}. \\t Vision type: {type(vision)}\")\n",
    "print(f\"Motor shape: {motor.shape}. \\t Motor type: {type(motor)}\")\n",
    "print(f\"Language shape: {language.shape}. \\t Language type: {type(language)}.\")\n",
    "print(f\"Mask shape: {mask.shape}. \\t Mask type: {type(mask)}.\")\n",
    "print(f\"Lang_mask shape: {lang_mask.shape}. \\t Lang_mask type: {type(lang_mask)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfda1a4-ff84-40b4-b409-95b52f3f5577",
   "metadata": {},
   "source": [
    "---\n",
    "Test the `ConvLSTMCell` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996bb02d-e7fa-4573-8e50-b9a6cf66c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h shape: torch.Size([35, 12, 128, 128])\n",
      "c shape: torch.Size([35, 12, 128, 128])\n",
      "inp_convs: torch.Size([35, 48, 128, 128])\n",
      "hid_convs: torch.Size([35, 48, 128, 128])\n",
      "ic: torch.Size([35, 12, 128, 128])\n",
      "h shape: torch.Size([35, 12, 128, 128])\n",
      "c shape: torch.Size([35, 12, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# test conv_lstm cell\n",
    "inp_channels = 3\n",
    "hid_size = 128  # desired hidden state size\n",
    "pad, r  = padding_fun(vision.shape[3],\n",
    "                      hid_size,\n",
    "                      config['s'],\n",
    "                      config['inp_kernel_size'],\n",
    "                      config['kd'])\n",
    "\n",
    "conv_lstm = ConvLSTMCell(inp_channels,\n",
    "                         config['h_channels'],\n",
    "                         config['inp_kernel_size'],\n",
    "                         config['hid_kernel_size'],\n",
    "                         inp_stride=config['s'],\n",
    "                         inp_padding=pad,\n",
    "                         ik_dilation=config['kd'])\n",
    "\n",
    "image_batch = vision[:, 0, :, :, :].to(torch.float32)\n",
    "\n",
    "img_height = image_batch.shape[2]\n",
    "img_width = image_batch.shape[3]\n",
    "h, c = conv_lstm.init_hidden_from_normal(vision.shape[0], (img_height, img_width))\n",
    "\n",
    "print(f\"h shape: {h.shape}\")\n",
    "print(f\"c shape: {c.shape}\")\n",
    "\n",
    "h, c = conv_lstm(image_batch, (h, c))\n",
    "\n",
    "print(f\"h shape: {h.shape}\")\n",
    "print(f\"c shape: {c.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63e397-0460-4f35-8381-9562e855a1a8",
   "metadata": {},
   "source": [
    "---\n",
    "Test the `PredictorCell` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823b93c3-1096-48c3-8ecc-9c0fc658e2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h shape: torch.Size([35, 12, 128, 128])\n",
      "c shape: torch.Size([35, 12, 128, 128])\n",
      "x_next shape: torch.Size([35, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "input_channels = 3  # the RGB channels\n",
    "hid_size = 128  # desired hidden state size\n",
    "pad, r  = padding_fun(vision.shape[3],\n",
    "                      hid_size,\n",
    "                      config['s'],\n",
    "                      config['inp_kernel_size'],\n",
    "                      config['kd'])\n",
    "conv_params = {'input_channels': input_channels,\n",
    "               'hidden_channels': config['h_channels'],\n",
    "               'inp_kernel_size': config['inp_kernel_size'],\n",
    "               'hid_kernel_size': config['hid_kernel_size'],\n",
    "               'inp_stride': config['s'],\n",
    "               'inp_padding': pad,\n",
    "               'ik_dilation': config['kd'],\n",
    "               'bias': True,\n",
    "              }\n",
    "\n",
    "pad_t, r_t = padding_fun(vision.shape[3],\n",
    "                         hid_size,\n",
    "                         config['s'],\n",
    "                         config['trans_kernel_size'],\n",
    "                         1)\n",
    "conv_params_t = {'kernel_size': config['trans_kernel_size'],\n",
    "                 'ik_dilation': 1,\n",
    "                 'inp_padding': pad_t,\n",
    "                 'output_padding': r_t,\n",
    "                 'bias': True,\n",
    "                }\n",
    "\n",
    "pred_cell = PredictorCell(conv_params, conv_params_t)\n",
    "\n",
    "image_batch = vision[:, 0, :, :, :].to(torch.float32)\n",
    "\n",
    "img_height = image_batch.shape[2]\n",
    "img_width = image_batch.shape[3]\n",
    "h, c = pred_cell.init_hidden(vision.shape[0], (img_height, img_width))\n",
    "\n",
    "x_next, h, c = pred_cell(image_batch, (h, c))\n",
    "\n",
    "print(f\"h shape: {h.shape}\")\n",
    "print(f\"c shape: {c.shape}\")\n",
    "print(f\"x_next shape: {x_next.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d380886-4300-4443-965e-c234870f5406",
   "metadata": {},
   "source": [
    "---\n",
    "Test the `Predictor` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf9504f-3d97-43e9-bf30-6b7161cbae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_batch shape: torch.Size([35, 3, 256, 256])\n",
      "pred_sequence shape: torch.Size([35, 50, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "input_channels = 3  # the RGB channels\n",
    "hid_size = 128  # desired hidden state size\n",
    "T = vision.shape[1]  # number of images to predict\n",
    "pad, r  = padding_fun(vision.shape[3],\n",
    "                      hid_size,\n",
    "                      config['s'],\n",
    "                      config['inp_kernel_size'],\n",
    "                      config['kd'])\n",
    "conv_params = {'input_channels': input_channels,\n",
    "               'hidden_channels': config['h_channels'],\n",
    "               'inp_kernel_size': config['inp_kernel_size'],\n",
    "               'hid_kernel_size': config['hid_kernel_size'],\n",
    "               'inp_stride': config['s'],\n",
    "               'inp_padding': pad,\n",
    "               'ik_dilation': config['kd'],\n",
    "               'bias': True,\n",
    "              }\n",
    "\n",
    "pad_t, r_t = padding_fun(vision.shape[3],\n",
    "                         hid_size,\n",
    "                         config['s'],\n",
    "                         config['trans_kernel_size'],\n",
    "                         1)\n",
    "conv_params_t = {'kernel_size': config['trans_kernel_size'],\n",
    "                 'ik_dilation': 1,\n",
    "                 'inp_padding': pad_t,\n",
    "                 'output_padding': r_t,\n",
    "                 'bias': True,\n",
    "                }\n",
    "image_batch = vision[:, 0, :, :, :].to(torch.float32)\n",
    "\n",
    "predictor = Predictor(conv_params, conv_params_t)\n",
    "\n",
    "pred_sequence = predictor(image_batch, T)\n",
    "\n",
    "print(f\"image_batch shape: {image_batch.shape}\")\n",
    "print(f\"pred_sequence shape: {pred_sequence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb8fcc-79cb-4819-8610-ac39dc1057d6",
   "metadata": {},
   "source": [
    "---\n",
    "### Perform backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e959588-38d6-4b45-aa32-623cd5a2911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "input_channels = 3  # the RGB channels\n",
    "hid_size = 128  # desired hidden state size\n",
    "T = vision.shape[1]  # number of images to predict\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "pad, r  = padding_fun(vision.shape[3],\n",
    "                      hid_size,\n",
    "                      config['s'],\n",
    "                      config['inp_kernel_size'],\n",
    "                      config['kd'])\n",
    "conv_params = {'input_channels': input_channels,\n",
    "               'hidden_channels': config['h_channels'],\n",
    "               'inp_kernel_size': config['inp_kernel_size'],\n",
    "               'hid_kernel_size': config['hid_kernel_size'],\n",
    "               'inp_stride': config['s'],\n",
    "               'inp_padding': pad,\n",
    "               'ik_dilation': config['kd'],\n",
    "               'bias': True,\n",
    "              }\n",
    "\n",
    "pad_t, r_t = padding_fun(vision.shape[3],\n",
    "                         hid_size,\n",
    "                         config['s'],\n",
    "                         config['trans_kernel_size'],\n",
    "                         1)\n",
    "conv_params_t = {'kernel_size': config['trans_kernel_size'],\n",
    "                 'ik_dilation': 1,\n",
    "                 'inp_padding': pad_t,\n",
    "                 'output_padding': r_t,\n",
    "                 'bias': True,\n",
    "                }\n",
    "\n",
    "predictor = Predictor(conv_params, conv_params_t).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(predictor.parameters(), lr=config['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b00755-4fa6-4f4e-90fe-22e742e468ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=0.3999221192465888\n",
      "Epoch 5, loss=2.3724786374304028\n"
     ]
    }
   ],
   "source": [
    "accum_loss = 0.0\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    for datum in data_loader:\n",
    "        vision, motor, language, mask, lang_mask = datum\n",
    "        vision = vision.to(device, dtype=torch.float32)\n",
    "        first_images_batch = vision[:, 0, :, :, :].detach().clone().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = predictor(first_images_batch, vision.shape[1])\n",
    "        loss = loss_fn(vision, predictions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accum_loss += loss.item()\n",
    "    losses.append(accum_loss / len(data_loader)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6e2751-101a-48a7-8208-56096d102121",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 23.46 GiB total capacity; 20.99 GiB already allocated; 47.12 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m vision \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m first_images_batch \u001b[38;5;241m=\u001b[39m vision[:, \u001b[38;5;241m0\u001b[39m, :, :, :]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_images_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m vision_ex \u001b[38;5;241m=\u001b[39m predictions[example_index, :, :, :, :]\n",
      "File \u001b[0;32m~/environments/soup/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/language_network/conv_lstm_test/torch/torch_conv_lstm.py:316\u001b[0m, in \u001b[0;36mPredictor.forward\u001b[0;34m(self, first_imgs, T, normal_init)\u001b[0m\n\u001b[1;32m    313\u001b[0m     pred_sequence \u001b[38;5;241m=\u001b[39m [first_imgs\u001b[38;5;241m.\u001b[39mclone()]\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, T):\n\u001b[0;32m--> 316\u001b[0m     img, h, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     pred_sequence\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(pred_sequence, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/environments/soup/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/language_network/conv_lstm_test/torch/torch_conv_lstm.py:267\u001b[0m, in \u001b[0;36mPredictorCell.forward\u001b[0;34m(self, input_tensor, cur_state)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Advance one time step.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m:param input_tensor: input tensor for the current time step\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m:rtype: tuple(torch.Tensor, torch.Tensor, torch.Tensor)\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    266\u001b[0m h_cur, c_cur \u001b[38;5;241m=\u001b[39m cur_state\n\u001b[0;32m--> 267\u001b[0m h_next, c_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_cur\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m next_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransp_conv(h_next)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m next_input, h_next, c_next\n",
      "File \u001b[0;32m~/environments/soup/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/language_network/conv_lstm_test/torch/torch_conv_lstm.py:171\u001b[0m, in \u001b[0;36mConvLSTMCell.forward\u001b[0;34m(self, input_tensor, cur_state)\u001b[0m\n\u001b[1;32m    169\u001b[0m i \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(ic \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_bias)\n\u001b[1;32m    170\u001b[0m f \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(fc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_bias)\n\u001b[0;32m--> 171\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43moc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m g \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mtanh(gc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_bias)\n\u001b[1;32m    173\u001b[0m c_next \u001b[38;5;241m=\u001b[39m f \u001b[38;5;241m*\u001b[39m c_cur \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m*\u001b[39m g\n",
      "File \u001b[0;32m~/environments/soup/lib/python3.8/site-packages/torch/nn/functional.py:1968\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"sigmoid(input) -> Tensor\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \n\u001b[1;32m   1964\u001b[0m \u001b[38;5;124;03m    Applies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\u001b[39;00m\n\u001b[1;32m   1965\u001b[0m \n\u001b[1;32m   1966\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.Sigmoid` for more details.\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 23.46 GiB total capacity; 20.99 GiB already allocated; 47.12 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# visualize some predictions\n",
    "example_index = 0\n",
    "datum = next(data_loader._get_iterator())\n",
    "vision, motor, language, mask, lang_mask = datum\n",
    "vision = vision.to(device, dtype=torch.float32)\n",
    "first_images_batch = vision[:, 0, :, :, :].detach().clone().to(device)\n",
    "predictions = predictor(first_images_batch, vision.shape[1])\n",
    "vision_ex = predictions[example_index, :, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864aea15-5166-4532-9b95-45c1eab047d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
